{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POSTTEST 8\n",
    "**NAMA  : DUTA VIRA PRADHANA DIPA**\n",
    "\n",
    "**NIM   : 2009106053**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Membaca Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.880</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.098</td>\n",
       "      <td>25.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.99680</td>\n",
       "      <td>3.20</td>\n",
       "      <td>0.68</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.8</td>\n",
       "      <td>0.760</td>\n",
       "      <td>0.04</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.092</td>\n",
       "      <td>15.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0.99700</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.65</td>\n",
       "      <td>9.8</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.2</td>\n",
       "      <td>0.280</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.075</td>\n",
       "      <td>17.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.99800</td>\n",
       "      <td>3.16</td>\n",
       "      <td>0.58</td>\n",
       "      <td>9.8</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.4</td>\n",
       "      <td>0.700</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.076</td>\n",
       "      <td>11.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.99780</td>\n",
       "      <td>3.51</td>\n",
       "      <td>0.56</td>\n",
       "      <td>9.4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1594</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.08</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.090</td>\n",
       "      <td>32.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.45</td>\n",
       "      <td>0.58</td>\n",
       "      <td>10.5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1595</th>\n",
       "      <td>5.9</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.10</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.062</td>\n",
       "      <td>39.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0.99512</td>\n",
       "      <td>3.52</td>\n",
       "      <td>0.76</td>\n",
       "      <td>11.2</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1596</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.13</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.076</td>\n",
       "      <td>29.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.99574</td>\n",
       "      <td>3.42</td>\n",
       "      <td>0.75</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1597</th>\n",
       "      <td>5.9</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.12</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.075</td>\n",
       "      <td>32.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>0.99547</td>\n",
       "      <td>3.57</td>\n",
       "      <td>0.71</td>\n",
       "      <td>10.2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1598</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.310</td>\n",
       "      <td>0.47</td>\n",
       "      <td>3.6</td>\n",
       "      <td>0.067</td>\n",
       "      <td>18.0</td>\n",
       "      <td>42.0</td>\n",
       "      <td>0.99549</td>\n",
       "      <td>3.39</td>\n",
       "      <td>0.66</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1599 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0               7.4             0.700         0.00             1.9      0.076   \n",
       "1               7.8             0.880         0.00             2.6      0.098   \n",
       "2               7.8             0.760         0.04             2.3      0.092   \n",
       "3              11.2             0.280         0.56             1.9      0.075   \n",
       "4               7.4             0.700         0.00             1.9      0.076   \n",
       "...             ...               ...          ...             ...        ...   \n",
       "1594            6.2             0.600         0.08             2.0      0.090   \n",
       "1595            5.9             0.550         0.10             2.2      0.062   \n",
       "1596            6.3             0.510         0.13             2.3      0.076   \n",
       "1597            5.9             0.645         0.12             2.0      0.075   \n",
       "1598            6.0             0.310         0.47             3.6      0.067   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                    11.0                  34.0  0.99780  3.51       0.56   \n",
       "1                    25.0                  67.0  0.99680  3.20       0.68   \n",
       "2                    15.0                  54.0  0.99700  3.26       0.65   \n",
       "3                    17.0                  60.0  0.99800  3.16       0.58   \n",
       "4                    11.0                  34.0  0.99780  3.51       0.56   \n",
       "...                   ...                   ...      ...   ...        ...   \n",
       "1594                 32.0                  44.0  0.99490  3.45       0.58   \n",
       "1595                 39.0                  51.0  0.99512  3.52       0.76   \n",
       "1596                 29.0                  40.0  0.99574  3.42       0.75   \n",
       "1597                 32.0                  44.0  0.99547  3.57       0.71   \n",
       "1598                 18.0                  42.0  0.99549  3.39       0.66   \n",
       "\n",
       "      alcohol  quality  \n",
       "0         9.4        5  \n",
       "1         9.8        5  \n",
       "2         9.8        5  \n",
       "3         9.8        6  \n",
       "4         9.4        5  \n",
       "...       ...      ...  \n",
       "1594     10.5        5  \n",
       "1595     11.2        6  \n",
       "1596     11.0        6  \n",
       "1597     10.2        5  \n",
       "1598     11.0        6  \n",
       "\n",
       "[1599 rows x 12 columns]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Dataset/winequality-red.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Menentukan record dan label pada dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[[\"fixed acidity\", \"volatile acidity\", \"citric acid\", \"residual sugar\", \"chlorides\", \"free sulfur dioxide\", \"total sulfur dioxide\", \"density\", \"pH\", \"sulphates\", \"alcohol\"]]\n",
    "Y = df[\"quality\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spliting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, random_state=0, test_size=.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Menetapkan jumlah kelas dan melakukan konversi label ke binary class matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = tf.keras.utils.to_categorical(y_train, 11)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Membuat struktur model sequential dan menampilkan summarynya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_139 (Dense)           (None, 500)               6000      \n",
      "                                                                 \n",
      " dense_140 (Dense)           (None, 350)               175350    \n",
      "                                                                 \n",
      " dense_141 (Dense)           (None, 250)               87750     \n",
      "                                                                 \n",
      " dense_142 (Dense)           (None, 250)               62750     \n",
      "                                                                 \n",
      " dense_143 (Dense)           (None, 125)               31375     \n",
      "                                                                 \n",
      " dense_144 (Dense)           (None, 125)               15750     \n",
      "                                                                 \n",
      " dense_145 (Dense)           (None, 100)               12600     \n",
      "                                                                 \n",
      " dense_146 (Dense)           (None, 11)                1111      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 392,686\n",
      "Trainable params: 392,686\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(500, activation=\"relu\", input_dim=11),\n",
    "\n",
    "    tf.keras.layers.Dense(350, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(250, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(250, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(125, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(125, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "\n",
    "    tf.keras.layers.Dense(11, activation=\"softmax\")\n",
    "])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Melakukan compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=tf.keras.losses.categorical_crossentropy,\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Melakukan training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/220\n",
      "37/37 - 2s - loss: 1.3670 - accuracy: 0.4246 - val_loss: 1.1211 - val_accuracy: 0.5250 - 2s/epoch - 44ms/step\n",
      "Epoch 2/220\n",
      "37/37 - 0s - loss: 1.2185 - accuracy: 0.4621 - val_loss: 1.1110 - val_accuracy: 0.5094 - 278ms/epoch - 8ms/step\n",
      "Epoch 3/220\n",
      "37/37 - 0s - loss: 1.1840 - accuracy: 0.4910 - val_loss: 1.2534 - val_accuracy: 0.4219 - 294ms/epoch - 8ms/step\n",
      "Epoch 4/220\n",
      "37/37 - 0s - loss: 1.1853 - accuracy: 0.4715 - val_loss: 1.1118 - val_accuracy: 0.4938 - 306ms/epoch - 8ms/step\n",
      "Epoch 5/220\n",
      "37/37 - 0s - loss: 1.1598 - accuracy: 0.4808 - val_loss: 1.1388 - val_accuracy: 0.4906 - 287ms/epoch - 8ms/step\n",
      "Epoch 6/220\n",
      "37/37 - 0s - loss: 1.1953 - accuracy: 0.4824 - val_loss: 1.1244 - val_accuracy: 0.5188 - 288ms/epoch - 8ms/step\n",
      "Epoch 7/220\n",
      "37/37 - 0s - loss: 1.1483 - accuracy: 0.4949 - val_loss: 1.1041 - val_accuracy: 0.4938 - 287ms/epoch - 8ms/step\n",
      "Epoch 8/220\n",
      "37/37 - 0s - loss: 1.1501 - accuracy: 0.4980 - val_loss: 1.0658 - val_accuracy: 0.5375 - 278ms/epoch - 8ms/step\n",
      "Epoch 9/220\n",
      "37/37 - 0s - loss: 1.1322 - accuracy: 0.4887 - val_loss: 1.0808 - val_accuracy: 0.5500 - 347ms/epoch - 9ms/step\n",
      "Epoch 10/220\n",
      "37/37 - 0s - loss: 1.1367 - accuracy: 0.4973 - val_loss: 1.0496 - val_accuracy: 0.5250 - 406ms/epoch - 11ms/step\n",
      "Epoch 11/220\n",
      "37/37 - 0s - loss: 1.1152 - accuracy: 0.5121 - val_loss: 1.0991 - val_accuracy: 0.4719 - 316ms/epoch - 9ms/step\n",
      "Epoch 12/220\n",
      "37/37 - 0s - loss: 1.1301 - accuracy: 0.5082 - val_loss: 1.0544 - val_accuracy: 0.5312 - 300ms/epoch - 8ms/step\n",
      "Epoch 13/220\n",
      "37/37 - 0s - loss: 1.0950 - accuracy: 0.5168 - val_loss: 1.0123 - val_accuracy: 0.5625 - 319ms/epoch - 9ms/step\n",
      "Epoch 14/220\n",
      "37/37 - 0s - loss: 1.0878 - accuracy: 0.5113 - val_loss: 1.0596 - val_accuracy: 0.4844 - 323ms/epoch - 9ms/step\n",
      "Epoch 15/220\n",
      "37/37 - 0s - loss: 1.0899 - accuracy: 0.5152 - val_loss: 1.0459 - val_accuracy: 0.5562 - 300ms/epoch - 8ms/step\n",
      "Epoch 16/220\n",
      "37/37 - 0s - loss: 1.0409 - accuracy: 0.5293 - val_loss: 0.9734 - val_accuracy: 0.5813 - 318ms/epoch - 9ms/step\n",
      "Epoch 17/220\n",
      "37/37 - 0s - loss: 1.0374 - accuracy: 0.5270 - val_loss: 0.9925 - val_accuracy: 0.5969 - 317ms/epoch - 9ms/step\n",
      "Epoch 18/220\n",
      "37/37 - 0s - loss: 1.0445 - accuracy: 0.5512 - val_loss: 1.0664 - val_accuracy: 0.5375 - 312ms/epoch - 8ms/step\n",
      "Epoch 19/220\n",
      "37/37 - 0s - loss: 1.0282 - accuracy: 0.5496 - val_loss: 1.1570 - val_accuracy: 0.5125 - 284ms/epoch - 8ms/step\n",
      "Epoch 20/220\n",
      "37/37 - 0s - loss: 1.0338 - accuracy: 0.5426 - val_loss: 1.0201 - val_accuracy: 0.5500 - 298ms/epoch - 8ms/step\n",
      "Epoch 21/220\n",
      "37/37 - 0s - loss: 1.0181 - accuracy: 0.5520 - val_loss: 0.9915 - val_accuracy: 0.5844 - 284ms/epoch - 8ms/step\n",
      "Epoch 22/220\n",
      "37/37 - 0s - loss: 0.9785 - accuracy: 0.5614 - val_loss: 1.0565 - val_accuracy: 0.5219 - 295ms/epoch - 8ms/step\n",
      "Epoch 23/220\n",
      "37/37 - 0s - loss: 1.0202 - accuracy: 0.5410 - val_loss: 1.0897 - val_accuracy: 0.4969 - 287ms/epoch - 8ms/step\n",
      "Epoch 24/220\n",
      "37/37 - 0s - loss: 0.9833 - accuracy: 0.5629 - val_loss: 0.9533 - val_accuracy: 0.6031 - 281ms/epoch - 8ms/step\n",
      "Epoch 25/220\n",
      "37/37 - 0s - loss: 0.9689 - accuracy: 0.5551 - val_loss: 0.9673 - val_accuracy: 0.5969 - 293ms/epoch - 8ms/step\n",
      "Epoch 26/220\n",
      "37/37 - 0s - loss: 1.0072 - accuracy: 0.5598 - val_loss: 1.0225 - val_accuracy: 0.6000 - 282ms/epoch - 8ms/step\n",
      "Epoch 27/220\n",
      "37/37 - 0s - loss: 0.9641 - accuracy: 0.5723 - val_loss: 1.0149 - val_accuracy: 0.5938 - 289ms/epoch - 8ms/step\n",
      "Epoch 28/220\n",
      "37/37 - 0s - loss: 0.9900 - accuracy: 0.5559 - val_loss: 1.0088 - val_accuracy: 0.5813 - 298ms/epoch - 8ms/step\n",
      "Epoch 29/220\n",
      "37/37 - 0s - loss: 0.9731 - accuracy: 0.5653 - val_loss: 1.0176 - val_accuracy: 0.5781 - 314ms/epoch - 8ms/step\n",
      "Epoch 30/220\n",
      "37/37 - 0s - loss: 0.9779 - accuracy: 0.5622 - val_loss: 1.0480 - val_accuracy: 0.6031 - 286ms/epoch - 8ms/step\n",
      "Epoch 31/220\n",
      "37/37 - 0s - loss: 0.9805 - accuracy: 0.5637 - val_loss: 0.9733 - val_accuracy: 0.5906 - 287ms/epoch - 8ms/step\n",
      "Epoch 32/220\n",
      "37/37 - 0s - loss: 0.9454 - accuracy: 0.5770 - val_loss: 0.9937 - val_accuracy: 0.6000 - 322ms/epoch - 9ms/step\n",
      "Epoch 33/220\n",
      "37/37 - 0s - loss: 0.9384 - accuracy: 0.5841 - val_loss: 0.9864 - val_accuracy: 0.5813 - 302ms/epoch - 8ms/step\n",
      "Epoch 34/220\n",
      "37/37 - 0s - loss: 0.9262 - accuracy: 0.5762 - val_loss: 0.9872 - val_accuracy: 0.5875 - 294ms/epoch - 8ms/step\n",
      "Epoch 35/220\n",
      "37/37 - 0s - loss: 0.9255 - accuracy: 0.5637 - val_loss: 0.9815 - val_accuracy: 0.6125 - 288ms/epoch - 8ms/step\n",
      "Epoch 36/220\n",
      "37/37 - 0s - loss: 0.9318 - accuracy: 0.5825 - val_loss: 0.9953 - val_accuracy: 0.6156 - 280ms/epoch - 8ms/step\n",
      "Epoch 37/220\n",
      "37/37 - 0s - loss: 0.9500 - accuracy: 0.5668 - val_loss: 1.0121 - val_accuracy: 0.5969 - 292ms/epoch - 8ms/step\n",
      "Epoch 38/220\n",
      "37/37 - 0s - loss: 0.9207 - accuracy: 0.5833 - val_loss: 1.0529 - val_accuracy: 0.5688 - 288ms/epoch - 8ms/step\n",
      "Epoch 39/220\n",
      "37/37 - 0s - loss: 0.9224 - accuracy: 0.5856 - val_loss: 0.9789 - val_accuracy: 0.6094 - 291ms/epoch - 8ms/step\n",
      "Epoch 40/220\n",
      "37/37 - 0s - loss: 0.9359 - accuracy: 0.5614 - val_loss: 0.9969 - val_accuracy: 0.6062 - 280ms/epoch - 8ms/step\n",
      "Epoch 41/220\n",
      "37/37 - 0s - loss: 0.9118 - accuracy: 0.5966 - val_loss: 0.9996 - val_accuracy: 0.6125 - 291ms/epoch - 8ms/step\n",
      "Epoch 42/220\n",
      "37/37 - 0s - loss: 0.9176 - accuracy: 0.5966 - val_loss: 1.0335 - val_accuracy: 0.5781 - 315ms/epoch - 9ms/step\n",
      "Epoch 43/220\n",
      "37/37 - 0s - loss: 0.9103 - accuracy: 0.5903 - val_loss: 0.9896 - val_accuracy: 0.5969 - 289ms/epoch - 8ms/step\n",
      "Epoch 44/220\n",
      "37/37 - 0s - loss: 0.9652 - accuracy: 0.5762 - val_loss: 1.1932 - val_accuracy: 0.5125 - 293ms/epoch - 8ms/step\n",
      "Epoch 45/220\n",
      "37/37 - 0s - loss: 0.9737 - accuracy: 0.5684 - val_loss: 0.9691 - val_accuracy: 0.6031 - 287ms/epoch - 8ms/step\n",
      "Epoch 46/220\n",
      "37/37 - 0s - loss: 0.9050 - accuracy: 0.5833 - val_loss: 0.9518 - val_accuracy: 0.6250 - 302ms/epoch - 8ms/step\n",
      "Epoch 47/220\n",
      "37/37 - 0s - loss: 0.8971 - accuracy: 0.5966 - val_loss: 0.9771 - val_accuracy: 0.6187 - 286ms/epoch - 8ms/step\n",
      "Epoch 48/220\n",
      "37/37 - 0s - loss: 0.9507 - accuracy: 0.5762 - val_loss: 1.0078 - val_accuracy: 0.5813 - 289ms/epoch - 8ms/step\n",
      "Epoch 49/220\n",
      "37/37 - 0s - loss: 0.9220 - accuracy: 0.5895 - val_loss: 1.0184 - val_accuracy: 0.6062 - 293ms/epoch - 8ms/step\n",
      "Epoch 50/220\n",
      "37/37 - 0s - loss: 0.8854 - accuracy: 0.6138 - val_loss: 0.9936 - val_accuracy: 0.6031 - 283ms/epoch - 8ms/step\n",
      "Epoch 51/220\n",
      "37/37 - 0s - loss: 0.9082 - accuracy: 0.5880 - val_loss: 0.9749 - val_accuracy: 0.6031 - 296ms/epoch - 8ms/step\n",
      "Epoch 52/220\n",
      "37/37 - 0s - loss: 0.9085 - accuracy: 0.6005 - val_loss: 0.9747 - val_accuracy: 0.5844 - 284ms/epoch - 8ms/step\n",
      "Epoch 53/220\n",
      "37/37 - 0s - loss: 0.8884 - accuracy: 0.6036 - val_loss: 0.9511 - val_accuracy: 0.6187 - 290ms/epoch - 8ms/step\n",
      "Epoch 54/220\n",
      "37/37 - 0s - loss: 0.8832 - accuracy: 0.6138 - val_loss: 0.9564 - val_accuracy: 0.6156 - 282ms/epoch - 8ms/step\n",
      "Epoch 55/220\n",
      "37/37 - 0s - loss: 0.8640 - accuracy: 0.6099 - val_loss: 0.9936 - val_accuracy: 0.5906 - 293ms/epoch - 8ms/step\n",
      "Epoch 56/220\n",
      "37/37 - 0s - loss: 0.8928 - accuracy: 0.5989 - val_loss: 1.0049 - val_accuracy: 0.5844 - 288ms/epoch - 8ms/step\n",
      "Epoch 57/220\n",
      "37/37 - 0s - loss: 0.8962 - accuracy: 0.6036 - val_loss: 1.0726 - val_accuracy: 0.5594 - 288ms/epoch - 8ms/step\n",
      "Epoch 58/220\n",
      "37/37 - 0s - loss: 0.8817 - accuracy: 0.5981 - val_loss: 1.1019 - val_accuracy: 0.5437 - 293ms/epoch - 8ms/step\n",
      "Epoch 59/220\n",
      "37/37 - 0s - loss: 0.8800 - accuracy: 0.6067 - val_loss: 1.0277 - val_accuracy: 0.6000 - 283ms/epoch - 8ms/step\n",
      "Epoch 60/220\n",
      "37/37 - 0s - loss: 0.8624 - accuracy: 0.6083 - val_loss: 1.0109 - val_accuracy: 0.5719 - 296ms/epoch - 8ms/step\n",
      "Epoch 61/220\n",
      "37/37 - 0s - loss: 0.8709 - accuracy: 0.5989 - val_loss: 0.9840 - val_accuracy: 0.6125 - 284ms/epoch - 8ms/step\n",
      "Epoch 62/220\n",
      "37/37 - 0s - loss: 0.8475 - accuracy: 0.5997 - val_loss: 1.0244 - val_accuracy: 0.5969 - 309ms/epoch - 8ms/step\n",
      "Epoch 63/220\n",
      "37/37 - 0s - loss: 0.8651 - accuracy: 0.6114 - val_loss: 1.0272 - val_accuracy: 0.6031 - 298ms/epoch - 8ms/step\n",
      "Epoch 64/220\n",
      "37/37 - 0s - loss: 0.8484 - accuracy: 0.6208 - val_loss: 1.0334 - val_accuracy: 0.5781 - 299ms/epoch - 8ms/step\n",
      "Epoch 65/220\n",
      "37/37 - 0s - loss: 0.8455 - accuracy: 0.6231 - val_loss: 1.0722 - val_accuracy: 0.5719 - 298ms/epoch - 8ms/step\n",
      "Epoch 66/220\n",
      "37/37 - 0s - loss: 0.8725 - accuracy: 0.6052 - val_loss: 1.0373 - val_accuracy: 0.6000 - 281ms/epoch - 8ms/step\n",
      "Epoch 67/220\n",
      "37/37 - 0s - loss: 0.8463 - accuracy: 0.6099 - val_loss: 1.0478 - val_accuracy: 0.5875 - 289ms/epoch - 8ms/step\n",
      "Epoch 68/220\n",
      "37/37 - 0s - loss: 0.8681 - accuracy: 0.6153 - val_loss: 1.0574 - val_accuracy: 0.5875 - 292ms/epoch - 8ms/step\n",
      "Epoch 69/220\n",
      "37/37 - 0s - loss: 0.8410 - accuracy: 0.6216 - val_loss: 0.9914 - val_accuracy: 0.6219 - 287ms/epoch - 8ms/step\n",
      "Epoch 70/220\n",
      "37/37 - 0s - loss: 0.8238 - accuracy: 0.6310 - val_loss: 1.0511 - val_accuracy: 0.6094 - 285ms/epoch - 8ms/step\n",
      "Epoch 71/220\n",
      "37/37 - 0s - loss: 0.8606 - accuracy: 0.6169 - val_loss: 1.0231 - val_accuracy: 0.6250 - 277ms/epoch - 7ms/step\n",
      "Epoch 72/220\n",
      "37/37 - 0s - loss: 0.8221 - accuracy: 0.6294 - val_loss: 1.1112 - val_accuracy: 0.5625 - 293ms/epoch - 8ms/step\n",
      "Epoch 73/220\n",
      "37/37 - 0s - loss: 0.8188 - accuracy: 0.6364 - val_loss: 1.1153 - val_accuracy: 0.5875 - 282ms/epoch - 8ms/step\n",
      "Epoch 74/220\n",
      "37/37 - 0s - loss: 0.8262 - accuracy: 0.6388 - val_loss: 1.0417 - val_accuracy: 0.6281 - 283ms/epoch - 8ms/step\n",
      "Epoch 75/220\n",
      "37/37 - 0s - loss: 0.8608 - accuracy: 0.6106 - val_loss: 1.0387 - val_accuracy: 0.5625 - 287ms/epoch - 8ms/step\n",
      "Epoch 76/220\n",
      "37/37 - 0s - loss: 0.8538 - accuracy: 0.6364 - val_loss: 0.9863 - val_accuracy: 0.6281 - 292ms/epoch - 8ms/step\n",
      "Epoch 77/220\n",
      "37/37 - 0s - loss: 0.8352 - accuracy: 0.6263 - val_loss: 1.1114 - val_accuracy: 0.6031 - 330ms/epoch - 9ms/step\n",
      "Epoch 78/220\n",
      "37/37 - 0s - loss: 0.8144 - accuracy: 0.6411 - val_loss: 1.0684 - val_accuracy: 0.6125 - 316ms/epoch - 9ms/step\n",
      "Epoch 79/220\n",
      "37/37 - 0s - loss: 0.8176 - accuracy: 0.6310 - val_loss: 1.0473 - val_accuracy: 0.5844 - 301ms/epoch - 8ms/step\n",
      "Epoch 80/220\n",
      "37/37 - 0s - loss: 0.8219 - accuracy: 0.6177 - val_loss: 1.0144 - val_accuracy: 0.6406 - 317ms/epoch - 9ms/step\n",
      "Epoch 81/220\n",
      "37/37 - 0s - loss: 0.7862 - accuracy: 0.6341 - val_loss: 1.1039 - val_accuracy: 0.6125 - 310ms/epoch - 8ms/step\n",
      "Epoch 82/220\n",
      "37/37 - 0s - loss: 0.7934 - accuracy: 0.6294 - val_loss: 1.0932 - val_accuracy: 0.6094 - 354ms/epoch - 10ms/step\n",
      "Epoch 83/220\n",
      "37/37 - 0s - loss: 0.7858 - accuracy: 0.6396 - val_loss: 1.0861 - val_accuracy: 0.6000 - 331ms/epoch - 9ms/step\n",
      "Epoch 84/220\n",
      "37/37 - 0s - loss: 0.7662 - accuracy: 0.6607 - val_loss: 1.0982 - val_accuracy: 0.6187 - 321ms/epoch - 9ms/step\n",
      "Epoch 85/220\n",
      "37/37 - 0s - loss: 0.7925 - accuracy: 0.6364 - val_loss: 1.0692 - val_accuracy: 0.6156 - 317ms/epoch - 9ms/step\n",
      "Epoch 86/220\n",
      "37/37 - 0s - loss: 0.7675 - accuracy: 0.6443 - val_loss: 1.1718 - val_accuracy: 0.5844 - 339ms/epoch - 9ms/step\n",
      "Epoch 87/220\n",
      "37/37 - 0s - loss: 0.7887 - accuracy: 0.6317 - val_loss: 1.1305 - val_accuracy: 0.5875 - 303ms/epoch - 8ms/step\n",
      "Epoch 88/220\n",
      "37/37 - 0s - loss: 0.8440 - accuracy: 0.6067 - val_loss: 1.0640 - val_accuracy: 0.5531 - 318ms/epoch - 9ms/step\n",
      "Epoch 89/220\n",
      "37/37 - 0s - loss: 0.8694 - accuracy: 0.6145 - val_loss: 1.1058 - val_accuracy: 0.5844 - 318ms/epoch - 9ms/step\n",
      "Epoch 90/220\n",
      "37/37 - 0s - loss: 0.8347 - accuracy: 0.6294 - val_loss: 1.0843 - val_accuracy: 0.6187 - 367ms/epoch - 10ms/step\n",
      "Epoch 91/220\n",
      "37/37 - 0s - loss: 0.7663 - accuracy: 0.6513 - val_loss: 1.1981 - val_accuracy: 0.6187 - 378ms/epoch - 10ms/step\n",
      "Epoch 92/220\n",
      "37/37 - 0s - loss: 0.7530 - accuracy: 0.6505 - val_loss: 1.1527 - val_accuracy: 0.5531 - 379ms/epoch - 10ms/step\n",
      "Epoch 93/220\n",
      "37/37 - 0s - loss: 0.7318 - accuracy: 0.6591 - val_loss: 1.0536 - val_accuracy: 0.6438 - 327ms/epoch - 9ms/step\n",
      "Epoch 94/220\n",
      "37/37 - 0s - loss: 0.7402 - accuracy: 0.6740 - val_loss: 1.0631 - val_accuracy: 0.6156 - 318ms/epoch - 9ms/step\n",
      "Epoch 95/220\n",
      "37/37 - 0s - loss: 0.7567 - accuracy: 0.6630 - val_loss: 1.0552 - val_accuracy: 0.6094 - 320ms/epoch - 9ms/step\n",
      "Epoch 96/220\n",
      "37/37 - 0s - loss: 0.7721 - accuracy: 0.6638 - val_loss: 1.1592 - val_accuracy: 0.5656 - 307ms/epoch - 8ms/step\n",
      "Epoch 97/220\n",
      "37/37 - 0s - loss: 0.7671 - accuracy: 0.6552 - val_loss: 1.1097 - val_accuracy: 0.6219 - 314ms/epoch - 8ms/step\n",
      "Epoch 98/220\n",
      "37/37 - 0s - loss: 0.7232 - accuracy: 0.6802 - val_loss: 1.2053 - val_accuracy: 0.6156 - 359ms/epoch - 10ms/step\n",
      "Epoch 99/220\n",
      "37/37 - 0s - loss: 0.7116 - accuracy: 0.6622 - val_loss: 1.1275 - val_accuracy: 0.6406 - 388ms/epoch - 10ms/step\n",
      "Epoch 100/220\n",
      "37/37 - 0s - loss: 0.6900 - accuracy: 0.6865 - val_loss: 1.1943 - val_accuracy: 0.6125 - 349ms/epoch - 9ms/step\n",
      "Epoch 101/220\n",
      "37/37 - 0s - loss: 0.7157 - accuracy: 0.6685 - val_loss: 1.2608 - val_accuracy: 0.6000 - 313ms/epoch - 8ms/step\n",
      "Epoch 102/220\n",
      "37/37 - 0s - loss: 0.6775 - accuracy: 0.6951 - val_loss: 1.1516 - val_accuracy: 0.6281 - 333ms/epoch - 9ms/step\n",
      "Epoch 103/220\n",
      "37/37 - 0s - loss: 0.7032 - accuracy: 0.6833 - val_loss: 1.2084 - val_accuracy: 0.6187 - 306ms/epoch - 8ms/step\n",
      "Epoch 104/220\n",
      "37/37 - 0s - loss: 0.6828 - accuracy: 0.6794 - val_loss: 1.2053 - val_accuracy: 0.6187 - 311ms/epoch - 8ms/step\n",
      "Epoch 105/220\n",
      "37/37 - 0s - loss: 0.6561 - accuracy: 0.6904 - val_loss: 1.4188 - val_accuracy: 0.5562 - 323ms/epoch - 9ms/step\n",
      "Epoch 106/220\n",
      "37/37 - 0s - loss: 0.6959 - accuracy: 0.6818 - val_loss: 1.2412 - val_accuracy: 0.6187 - 393ms/epoch - 11ms/step\n",
      "Epoch 107/220\n",
      "37/37 - 0s - loss: 0.6814 - accuracy: 0.6998 - val_loss: 1.2400 - val_accuracy: 0.5844 - 414ms/epoch - 11ms/step\n",
      "Epoch 108/220\n",
      "37/37 - 0s - loss: 0.7001 - accuracy: 0.6787 - val_loss: 1.1983 - val_accuracy: 0.5594 - 443ms/epoch - 12ms/step\n",
      "Epoch 109/220\n",
      "37/37 - 0s - loss: 0.6521 - accuracy: 0.7131 - val_loss: 1.2427 - val_accuracy: 0.5875 - 445ms/epoch - 12ms/step\n",
      "Epoch 110/220\n",
      "37/37 - 0s - loss: 0.6552 - accuracy: 0.6935 - val_loss: 1.2215 - val_accuracy: 0.6125 - 485ms/epoch - 13ms/step\n",
      "Epoch 111/220\n",
      "37/37 - 0s - loss: 0.6638 - accuracy: 0.7052 - val_loss: 1.3178 - val_accuracy: 0.5844 - 332ms/epoch - 9ms/step\n",
      "Epoch 112/220\n",
      "37/37 - 0s - loss: 0.6758 - accuracy: 0.6755 - val_loss: 1.5193 - val_accuracy: 0.5188 - 324ms/epoch - 9ms/step\n",
      "Epoch 113/220\n",
      "37/37 - 0s - loss: 0.6977 - accuracy: 0.6747 - val_loss: 1.2101 - val_accuracy: 0.6125 - 336ms/epoch - 9ms/step\n",
      "Epoch 114/220\n",
      "37/37 - 0s - loss: 0.5933 - accuracy: 0.7287 - val_loss: 1.3685 - val_accuracy: 0.6469 - 369ms/epoch - 10ms/step\n",
      "Epoch 115/220\n",
      "37/37 - 0s - loss: 0.5938 - accuracy: 0.7279 - val_loss: 1.3760 - val_accuracy: 0.6281 - 346ms/epoch - 9ms/step\n",
      "Epoch 116/220\n",
      "37/37 - 0s - loss: 0.5840 - accuracy: 0.7240 - val_loss: 1.3863 - val_accuracy: 0.6187 - 300ms/epoch - 8ms/step\n",
      "Epoch 117/220\n",
      "37/37 - 0s - loss: 0.6241 - accuracy: 0.7013 - val_loss: 1.5273 - val_accuracy: 0.6062 - 321ms/epoch - 9ms/step\n",
      "Epoch 118/220\n",
      "37/37 - 0s - loss: 0.6122 - accuracy: 0.7224 - val_loss: 1.3184 - val_accuracy: 0.6156 - 288ms/epoch - 8ms/step\n",
      "Epoch 119/220\n",
      "37/37 - 0s - loss: 0.5923 - accuracy: 0.7263 - val_loss: 1.2669 - val_accuracy: 0.6187 - 331ms/epoch - 9ms/step\n",
      "Epoch 120/220\n",
      "37/37 - 0s - loss: 0.6293 - accuracy: 0.7115 - val_loss: 1.1751 - val_accuracy: 0.6000 - 476ms/epoch - 13ms/step\n",
      "Epoch 121/220\n",
      "37/37 - 0s - loss: 0.6478 - accuracy: 0.6974 - val_loss: 1.2176 - val_accuracy: 0.6219 - 389ms/epoch - 11ms/step\n",
      "Epoch 122/220\n",
      "37/37 - 0s - loss: 0.5964 - accuracy: 0.7240 - val_loss: 1.3224 - val_accuracy: 0.6156 - 396ms/epoch - 11ms/step\n",
      "Epoch 123/220\n",
      "37/37 - 0s - loss: 0.5570 - accuracy: 0.7381 - val_loss: 1.3750 - val_accuracy: 0.6094 - 334ms/epoch - 9ms/step\n",
      "Epoch 124/220\n",
      "37/37 - 0s - loss: 0.5379 - accuracy: 0.7631 - val_loss: 1.3853 - val_accuracy: 0.6000 - 346ms/epoch - 9ms/step\n",
      "Epoch 125/220\n",
      "37/37 - 0s - loss: 0.5265 - accuracy: 0.7608 - val_loss: 1.5583 - val_accuracy: 0.5969 - 408ms/epoch - 11ms/step\n",
      "Epoch 126/220\n",
      "37/37 - 1s - loss: 0.5842 - accuracy: 0.7248 - val_loss: 1.3127 - val_accuracy: 0.5969 - 570ms/epoch - 15ms/step\n",
      "Epoch 127/220\n",
      "37/37 - 1s - loss: 0.5684 - accuracy: 0.7529 - val_loss: 1.4058 - val_accuracy: 0.6000 - 523ms/epoch - 14ms/step\n",
      "Epoch 128/220\n",
      "37/37 - 0s - loss: 0.5184 - accuracy: 0.7615 - val_loss: 1.5431 - val_accuracy: 0.5875 - 341ms/epoch - 9ms/step\n",
      "Epoch 129/220\n",
      "37/37 - 0s - loss: 0.5787 - accuracy: 0.7357 - val_loss: 1.4402 - val_accuracy: 0.5781 - 444ms/epoch - 12ms/step\n",
      "Epoch 130/220\n",
      "37/37 - 0s - loss: 0.5851 - accuracy: 0.7256 - val_loss: 1.3271 - val_accuracy: 0.5750 - 403ms/epoch - 11ms/step\n",
      "Epoch 131/220\n",
      "37/37 - 0s - loss: 0.5381 - accuracy: 0.7459 - val_loss: 1.3665 - val_accuracy: 0.5938 - 313ms/epoch - 8ms/step\n",
      "Epoch 132/220\n",
      "37/37 - 0s - loss: 0.5615 - accuracy: 0.7482 - val_loss: 1.4217 - val_accuracy: 0.5781 - 299ms/epoch - 8ms/step\n",
      "Epoch 133/220\n",
      "37/37 - 0s - loss: 0.5170 - accuracy: 0.7764 - val_loss: 1.4805 - val_accuracy: 0.5938 - 293ms/epoch - 8ms/step\n",
      "Epoch 134/220\n",
      "37/37 - 0s - loss: 0.5141 - accuracy: 0.7686 - val_loss: 1.4778 - val_accuracy: 0.5906 - 287ms/epoch - 8ms/step\n",
      "Epoch 135/220\n",
      "37/37 - 0s - loss: 0.5009 - accuracy: 0.7639 - val_loss: 1.5918 - val_accuracy: 0.5750 - 294ms/epoch - 8ms/step\n",
      "Epoch 136/220\n",
      "37/37 - 0s - loss: 0.4670 - accuracy: 0.7889 - val_loss: 1.5573 - val_accuracy: 0.6219 - 297ms/epoch - 8ms/step\n",
      "Epoch 137/220\n",
      "37/37 - 0s - loss: 0.5216 - accuracy: 0.7725 - val_loss: 1.5375 - val_accuracy: 0.6187 - 327ms/epoch - 9ms/step\n",
      "Epoch 138/220\n",
      "37/37 - 0s - loss: 0.4825 - accuracy: 0.7905 - val_loss: 1.6241 - val_accuracy: 0.5750 - 347ms/epoch - 9ms/step\n",
      "Epoch 139/220\n",
      "37/37 - 0s - loss: 0.5037 - accuracy: 0.7686 - val_loss: 1.6547 - val_accuracy: 0.5781 - 323ms/epoch - 9ms/step\n",
      "Epoch 140/220\n",
      "37/37 - 0s - loss: 0.4526 - accuracy: 0.8053 - val_loss: 1.6414 - val_accuracy: 0.6062 - 317ms/epoch - 9ms/step\n",
      "Epoch 141/220\n",
      "37/37 - 0s - loss: 0.4412 - accuracy: 0.8155 - val_loss: 1.9322 - val_accuracy: 0.5250 - 298ms/epoch - 8ms/step\n",
      "Epoch 142/220\n",
      "37/37 - 0s - loss: 0.4672 - accuracy: 0.7959 - val_loss: 1.6652 - val_accuracy: 0.5781 - 388ms/epoch - 10ms/step\n",
      "Epoch 143/220\n",
      "37/37 - 0s - loss: 0.4466 - accuracy: 0.7959 - val_loss: 1.7283 - val_accuracy: 0.5969 - 320ms/epoch - 9ms/step\n",
      "Epoch 144/220\n",
      "37/37 - 0s - loss: 0.5318 - accuracy: 0.7686 - val_loss: 1.4992 - val_accuracy: 0.5875 - 325ms/epoch - 9ms/step\n",
      "Epoch 145/220\n",
      "37/37 - 0s - loss: 0.5073 - accuracy: 0.7787 - val_loss: 1.5096 - val_accuracy: 0.5906 - 439ms/epoch - 12ms/step\n",
      "Epoch 146/220\n",
      "37/37 - 0s - loss: 0.4419 - accuracy: 0.8022 - val_loss: 1.7082 - val_accuracy: 0.6062 - 293ms/epoch - 8ms/step\n",
      "Epoch 147/220\n",
      "37/37 - 0s - loss: 0.3741 - accuracy: 0.8468 - val_loss: 1.6790 - val_accuracy: 0.5938 - 289ms/epoch - 8ms/step\n",
      "Epoch 148/220\n",
      "37/37 - 0s - loss: 0.4219 - accuracy: 0.8124 - val_loss: 1.7004 - val_accuracy: 0.5938 - 296ms/epoch - 8ms/step\n",
      "Epoch 149/220\n",
      "37/37 - 0s - loss: 0.4172 - accuracy: 0.8210 - val_loss: 1.6439 - val_accuracy: 0.6313 - 287ms/epoch - 8ms/step\n",
      "Epoch 150/220\n",
      "37/37 - 0s - loss: 0.3791 - accuracy: 0.8280 - val_loss: 1.7143 - val_accuracy: 0.5938 - 294ms/epoch - 8ms/step\n",
      "Epoch 151/220\n",
      "37/37 - 0s - loss: 0.3662 - accuracy: 0.8350 - val_loss: 1.7977 - val_accuracy: 0.6094 - 287ms/epoch - 8ms/step\n",
      "Epoch 152/220\n",
      "37/37 - 0s - loss: 0.3746 - accuracy: 0.8288 - val_loss: 1.7641 - val_accuracy: 0.6156 - 308ms/epoch - 8ms/step\n",
      "Epoch 153/220\n",
      "37/37 - 0s - loss: 0.3718 - accuracy: 0.8405 - val_loss: 2.0611 - val_accuracy: 0.5938 - 291ms/epoch - 8ms/step\n",
      "Epoch 154/220\n",
      "37/37 - 0s - loss: 0.3787 - accuracy: 0.8468 - val_loss: 1.7782 - val_accuracy: 0.6344 - 289ms/epoch - 8ms/step\n",
      "Epoch 155/220\n",
      "37/37 - 0s - loss: 0.3499 - accuracy: 0.8522 - val_loss: 1.8569 - val_accuracy: 0.5938 - 296ms/epoch - 8ms/step\n",
      "Epoch 156/220\n",
      "37/37 - 0s - loss: 0.3569 - accuracy: 0.8428 - val_loss: 1.9331 - val_accuracy: 0.5906 - 287ms/epoch - 8ms/step\n",
      "Epoch 157/220\n",
      "37/37 - 0s - loss: 0.4075 - accuracy: 0.8327 - val_loss: 1.7427 - val_accuracy: 0.6438 - 292ms/epoch - 8ms/step\n",
      "Epoch 158/220\n",
      "37/37 - 0s - loss: 0.3763 - accuracy: 0.8475 - val_loss: 1.8411 - val_accuracy: 0.6125 - 288ms/epoch - 8ms/step\n",
      "Epoch 159/220\n",
      "37/37 - 0s - loss: 0.6664 - accuracy: 0.7271 - val_loss: 1.2677 - val_accuracy: 0.5656 - 294ms/epoch - 8ms/step\n",
      "Epoch 160/220\n",
      "37/37 - 0s - loss: 0.5494 - accuracy: 0.7701 - val_loss: 1.3180 - val_accuracy: 0.6000 - 290ms/epoch - 8ms/step\n",
      "Epoch 161/220\n",
      "37/37 - 0s - loss: 0.3483 - accuracy: 0.8640 - val_loss: 1.5630 - val_accuracy: 0.5938 - 285ms/epoch - 8ms/step\n",
      "Epoch 162/220\n",
      "37/37 - 0s - loss: 0.3651 - accuracy: 0.8436 - val_loss: 1.5364 - val_accuracy: 0.6031 - 293ms/epoch - 8ms/step\n",
      "Epoch 163/220\n",
      "37/37 - 0s - loss: 0.3522 - accuracy: 0.8514 - val_loss: 1.6513 - val_accuracy: 0.6313 - 290ms/epoch - 8ms/step\n",
      "Epoch 164/220\n",
      "37/37 - 0s - loss: 0.3475 - accuracy: 0.8546 - val_loss: 1.7690 - val_accuracy: 0.5750 - 294ms/epoch - 8ms/step\n",
      "Epoch 165/220\n",
      "37/37 - 0s - loss: 0.3510 - accuracy: 0.8436 - val_loss: 1.6661 - val_accuracy: 0.6094 - 287ms/epoch - 8ms/step\n",
      "Epoch 166/220\n",
      "37/37 - 0s - loss: 0.3937 - accuracy: 0.8217 - val_loss: 1.6437 - val_accuracy: 0.6219 - 294ms/epoch - 8ms/step\n",
      "Epoch 167/220\n",
      "37/37 - 0s - loss: 0.3103 - accuracy: 0.8733 - val_loss: 1.8380 - val_accuracy: 0.6313 - 287ms/epoch - 8ms/step\n",
      "Epoch 168/220\n",
      "37/37 - 0s - loss: 0.2937 - accuracy: 0.8765 - val_loss: 1.7622 - val_accuracy: 0.6531 - 285ms/epoch - 8ms/step\n",
      "Epoch 169/220\n",
      "37/37 - 0s - loss: 0.3108 - accuracy: 0.8632 - val_loss: 1.8771 - val_accuracy: 0.5906 - 284ms/epoch - 8ms/step\n",
      "Epoch 170/220\n",
      "37/37 - 0s - loss: 0.2999 - accuracy: 0.8686 - val_loss: 1.9538 - val_accuracy: 0.6375 - 279ms/epoch - 8ms/step\n",
      "Epoch 171/220\n",
      "37/37 - 0s - loss: 0.3014 - accuracy: 0.8741 - val_loss: 1.8233 - val_accuracy: 0.6438 - 299ms/epoch - 8ms/step\n",
      "Epoch 172/220\n",
      "37/37 - 0s - loss: 0.3063 - accuracy: 0.8772 - val_loss: 1.9077 - val_accuracy: 0.6187 - 290ms/epoch - 8ms/step\n",
      "Epoch 173/220\n",
      "37/37 - 0s - loss: 0.3997 - accuracy: 0.8358 - val_loss: 2.0099 - val_accuracy: 0.5781 - 283ms/epoch - 8ms/step\n",
      "Epoch 174/220\n",
      "37/37 - 0s - loss: 0.4080 - accuracy: 0.8296 - val_loss: 1.7935 - val_accuracy: 0.6062 - 285ms/epoch - 8ms/step\n",
      "Epoch 175/220\n",
      "37/37 - 0s - loss: 0.3283 - accuracy: 0.8679 - val_loss: 1.8077 - val_accuracy: 0.6156 - 277ms/epoch - 7ms/step\n",
      "Epoch 176/220\n",
      "37/37 - 0s - loss: 0.2486 - accuracy: 0.8984 - val_loss: 2.0014 - val_accuracy: 0.6031 - 304ms/epoch - 8ms/step\n",
      "Epoch 177/220\n",
      "37/37 - 0s - loss: 0.2418 - accuracy: 0.8984 - val_loss: 1.9849 - val_accuracy: 0.6125 - 280ms/epoch - 8ms/step\n",
      "Epoch 178/220\n",
      "37/37 - 0s - loss: 0.3635 - accuracy: 0.8522 - val_loss: 2.0369 - val_accuracy: 0.5719 - 306ms/epoch - 8ms/step\n",
      "Epoch 179/220\n",
      "37/37 - 0s - loss: 0.3407 - accuracy: 0.8647 - val_loss: 1.9285 - val_accuracy: 0.6281 - 290ms/epoch - 8ms/step\n",
      "Epoch 180/220\n",
      "37/37 - 0s - loss: 0.2691 - accuracy: 0.8905 - val_loss: 1.9263 - val_accuracy: 0.6219 - 277ms/epoch - 7ms/step\n",
      "Epoch 181/220\n",
      "37/37 - 0s - loss: 0.2548 - accuracy: 0.8944 - val_loss: 2.1588 - val_accuracy: 0.6094 - 287ms/epoch - 8ms/step\n",
      "Epoch 182/220\n",
      "37/37 - 0s - loss: 0.2932 - accuracy: 0.8882 - val_loss: 1.9575 - val_accuracy: 0.5906 - 282ms/epoch - 8ms/step\n",
      "Epoch 183/220\n",
      "37/37 - 0s - loss: 0.2920 - accuracy: 0.8788 - val_loss: 1.9552 - val_accuracy: 0.6531 - 291ms/epoch - 8ms/step\n",
      "Epoch 184/220\n",
      "37/37 - 0s - loss: 0.2143 - accuracy: 0.9109 - val_loss: 2.0836 - val_accuracy: 0.6375 - 280ms/epoch - 8ms/step\n",
      "Epoch 185/220\n",
      "37/37 - 0s - loss: 0.2720 - accuracy: 0.8890 - val_loss: 2.1572 - val_accuracy: 0.5719 - 290ms/epoch - 8ms/step\n",
      "Epoch 186/220\n",
      "37/37 - 0s - loss: 0.3661 - accuracy: 0.8483 - val_loss: 1.8267 - val_accuracy: 0.6219 - 282ms/epoch - 8ms/step\n",
      "Epoch 187/220\n",
      "37/37 - 0s - loss: 0.2317 - accuracy: 0.9023 - val_loss: 2.1131 - val_accuracy: 0.6094 - 284ms/epoch - 8ms/step\n",
      "Epoch 188/220\n",
      "37/37 - 0s - loss: 0.2058 - accuracy: 0.9218 - val_loss: 2.1857 - val_accuracy: 0.6344 - 287ms/epoch - 8ms/step\n",
      "Epoch 189/220\n",
      "37/37 - 0s - loss: 0.2573 - accuracy: 0.8851 - val_loss: 2.0362 - val_accuracy: 0.6094 - 282ms/epoch - 8ms/step\n",
      "Epoch 190/220\n",
      "37/37 - 0s - loss: 0.2609 - accuracy: 0.8890 - val_loss: 2.1563 - val_accuracy: 0.6000 - 291ms/epoch - 8ms/step\n",
      "Epoch 191/220\n",
      "37/37 - 0s - loss: 0.2558 - accuracy: 0.9023 - val_loss: 2.1360 - val_accuracy: 0.6219 - 281ms/epoch - 8ms/step\n",
      "Epoch 192/220\n",
      "37/37 - 0s - loss: 0.2211 - accuracy: 0.9054 - val_loss: 2.1223 - val_accuracy: 0.6438 - 293ms/epoch - 8ms/step\n",
      "Epoch 193/220\n",
      "37/37 - 0s - loss: 0.1764 - accuracy: 0.9242 - val_loss: 2.3898 - val_accuracy: 0.6500 - 287ms/epoch - 8ms/step\n",
      "Epoch 194/220\n",
      "37/37 - 0s - loss: 0.2946 - accuracy: 0.8843 - val_loss: 2.3200 - val_accuracy: 0.5406 - 281ms/epoch - 8ms/step\n",
      "Epoch 195/220\n",
      "37/37 - 0s - loss: 0.5644 - accuracy: 0.7826 - val_loss: 1.7413 - val_accuracy: 0.6062 - 288ms/epoch - 8ms/step\n",
      "Epoch 196/220\n",
      "37/37 - 0s - loss: 0.3409 - accuracy: 0.8804 - val_loss: 1.6484 - val_accuracy: 0.6344 - 282ms/epoch - 8ms/step\n",
      "Epoch 197/220\n",
      "37/37 - 0s - loss: 0.1995 - accuracy: 0.9257 - val_loss: 2.0238 - val_accuracy: 0.6219 - 306ms/epoch - 8ms/step\n",
      "Epoch 198/220\n",
      "37/37 - 0s - loss: 0.2001 - accuracy: 0.9171 - val_loss: 2.1126 - val_accuracy: 0.6500 - 284ms/epoch - 8ms/step\n",
      "Epoch 199/220\n",
      "37/37 - 0s - loss: 0.2492 - accuracy: 0.8944 - val_loss: 2.0064 - val_accuracy: 0.6687 - 295ms/epoch - 8ms/step\n",
      "Epoch 200/220\n",
      "37/37 - 0s - loss: 0.2289 - accuracy: 0.9023 - val_loss: 2.1076 - val_accuracy: 0.6531 - 289ms/epoch - 8ms/step\n",
      "Epoch 201/220\n",
      "37/37 - 0s - loss: 0.1573 - accuracy: 0.9414 - val_loss: 2.3591 - val_accuracy: 0.6313 - 284ms/epoch - 8ms/step\n",
      "Epoch 202/220\n",
      "37/37 - 0s - loss: 0.1652 - accuracy: 0.9390 - val_loss: 2.4980 - val_accuracy: 0.6375 - 371ms/epoch - 10ms/step\n",
      "Epoch 203/220\n",
      "37/37 - 0s - loss: 0.1786 - accuracy: 0.9289 - val_loss: 2.7076 - val_accuracy: 0.6125 - 287ms/epoch - 8ms/step\n",
      "Epoch 204/220\n",
      "37/37 - 0s - loss: 0.2176 - accuracy: 0.9171 - val_loss: 2.2672 - val_accuracy: 0.6125 - 293ms/epoch - 8ms/step\n",
      "Epoch 205/220\n",
      "37/37 - 0s - loss: 0.2339 - accuracy: 0.9054 - val_loss: 2.2113 - val_accuracy: 0.6375 - 294ms/epoch - 8ms/step\n",
      "Epoch 206/220\n",
      "37/37 - 0s - loss: 0.2182 - accuracy: 0.9163 - val_loss: 2.2968 - val_accuracy: 0.5813 - 285ms/epoch - 8ms/step\n",
      "Epoch 207/220\n",
      "37/37 - 0s - loss: 0.1883 - accuracy: 0.9195 - val_loss: 2.3372 - val_accuracy: 0.6062 - 290ms/epoch - 8ms/step\n",
      "Epoch 208/220\n",
      "37/37 - 0s - loss: 0.2160 - accuracy: 0.9179 - val_loss: 2.5029 - val_accuracy: 0.5781 - 296ms/epoch - 8ms/step\n",
      "Epoch 209/220\n",
      "37/37 - 0s - loss: 0.2117 - accuracy: 0.9156 - val_loss: 2.0854 - val_accuracy: 0.6375 - 295ms/epoch - 8ms/step\n",
      "Epoch 210/220\n",
      "37/37 - 0s - loss: 0.1705 - accuracy: 0.9335 - val_loss: 2.4337 - val_accuracy: 0.6344 - 281ms/epoch - 8ms/step\n",
      "Epoch 211/220\n",
      "37/37 - 0s - loss: 0.1629 - accuracy: 0.9351 - val_loss: 2.5363 - val_accuracy: 0.5969 - 296ms/epoch - 8ms/step\n",
      "Epoch 212/220\n",
      "37/37 - 0s - loss: 0.2232 - accuracy: 0.9148 - val_loss: 2.2629 - val_accuracy: 0.6313 - 292ms/epoch - 8ms/step\n",
      "Epoch 213/220\n",
      "37/37 - 0s - loss: 0.2205 - accuracy: 0.9062 - val_loss: 2.1657 - val_accuracy: 0.6531 - 282ms/epoch - 8ms/step\n",
      "Epoch 214/220\n",
      "37/37 - 0s - loss: 0.1974 - accuracy: 0.9242 - val_loss: 2.3747 - val_accuracy: 0.6219 - 289ms/epoch - 8ms/step\n",
      "Epoch 215/220\n",
      "37/37 - 0s - loss: 0.2017 - accuracy: 0.9163 - val_loss: 2.3007 - val_accuracy: 0.6187 - 280ms/epoch - 8ms/step\n",
      "Epoch 216/220\n",
      "37/37 - 0s - loss: 0.2332 - accuracy: 0.9085 - val_loss: 2.2418 - val_accuracy: 0.6250 - 299ms/epoch - 8ms/step\n",
      "Epoch 217/220\n",
      "37/37 - 0s - loss: 0.1859 - accuracy: 0.9226 - val_loss: 2.3758 - val_accuracy: 0.6313 - 287ms/epoch - 8ms/step\n",
      "Epoch 218/220\n",
      "37/37 - 0s - loss: 0.1396 - accuracy: 0.9375 - val_loss: 2.6180 - val_accuracy: 0.6062 - 328ms/epoch - 9ms/step\n",
      "Epoch 219/220\n",
      "37/37 - 0s - loss: 0.2207 - accuracy: 0.9148 - val_loss: 2.4839 - val_accuracy: 0.5750 - 315ms/epoch - 9ms/step\n",
      "Epoch 220/220\n",
      "37/37 - 0s - loss: 0.1915 - accuracy: 0.9218 - val_loss: 2.3221 - val_accuracy: 0.6156 - 285ms/epoch - 8ms/step\n"
     ]
    }
   ],
   "source": [
    "fit = model.fit(\n",
    "    x=x_train, y=y_train,\n",
    "    validation_data=(x_test, y_test),\n",
    "    epochs=220,\n",
    "    steps_per_epoch=37,\n",
    "    verbose=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Menampilkan Akurasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 61.56250238418579 %\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test accuracy:', score[1]*100, '%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a9cff5a362bc38ef45d817ae74b1af54d6a076e3d773891282bce078b815ba34"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
